{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16957500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jsonlines\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import FastText\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import fasttext.util\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "146eeac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  i feel awful about it too because it s my job ...      0\n",
      "1                              im alone i feel awful      0\n",
      "2  ive probably mentioned this before but i reall...      1\n",
      "3           i was feeling a little low few days back      0\n",
      "4  i beleive that i am much more sensitive to oth...      2\n"
     ]
    }
   ],
   "source": [
    "# Загрузим данные\n",
    "data = []\n",
    "\n",
    "with jsonlines.open('C:/data.jsonl', 'r') as reader:\n",
    "    for line in reader:\n",
    "        data.append(line)\n",
    "        \n",
    "# Создаем DataFrame из данных\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Предварительный анализ данных\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a5b1a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0673ae55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_12684/1455451783.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text'] = df['text'].str.replace(r'[^a-zA-Z]', ' ')\n"
     ]
    }
   ],
   "source": [
    "# Очистка текста от лишних символов и токенизация\n",
    "\n",
    "df['text'] = df['text'].str.replace(r'[^a-zA-Z]', ' ')\n",
    "df['text'] = df['text'].str.lower()\n",
    "df['text'] = df['text'].apply(word_tokenize)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join(word for word in text if word not in stop_words)\n",
    "\n",
    "df['text'] = df['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2413024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Векторизация текста с использованием TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf_vectorizer.fit_transform(df['text'])\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a58ac98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Разделение данных на обучающий и тестовый наборы и обучение модели\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93315f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8988322173496703\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94     24504\n",
      "           1       0.92      0.93      0.93     28247\n",
      "           2       0.80      0.77      0.79      6853\n",
      "           3       0.89      0.90      0.90     11339\n",
      "           4       0.85      0.85      0.85      9376\n",
      "           5       0.79      0.71      0.75      3043\n",
      "\n",
      "    accuracy                           0.90     83362\n",
      "   macro avg       0.86      0.85      0.86     83362\n",
      "weighted avg       0.90      0.90      0.90     83362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Оценка модели и предоставление метрик\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'F1 Score: {f1}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3a4f1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "fasttext_model = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74ffc131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  i feel awful about it too because it s my job ...      0\n",
      "1                              im alone i feel awful      0\n",
      "2  ive probably mentioned this before but i reall...      1\n",
      "3           i was feeling a little low few days back      0\n",
      "4  i beleive that i am much more sensitive to oth...      2\n"
     ]
    }
   ],
   "source": [
    "# Загрузим данные\n",
    "data = []\n",
    "\n",
    "with jsonlines.open('C:/data.jsonl', 'r') as reader:\n",
    "    for line in reader:\n",
    "        data.append(line)\n",
    "        \n",
    "# Создаем DataFrame из данных\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Предварительный анализ данных\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a2a1363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Функция для создания эмбеддингов для текстов\n",
    "def text_to_vector(text):\n",
    "    words = text.split()\n",
    "    vectors = [fasttext_model[word] for word in words if word in fasttext_model]\n",
    "    if not vectors:\n",
    "        return [0] * fasttext_model.vector_size\n",
    "    return sum(vectors) / len(vectors)\n",
    "\n",
    "# Преобразование текстов в эмбеддинги\n",
    "df['text_vector'] = df['text'].apply(text_to_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f6c8f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_12684/1317166703.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  X = torch.Tensor(df['text_vector'].tolist())\n"
     ]
    }
   ],
   "source": [
    "# Класс для простой нейронной сети + LSTM слой\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Преобразование данных в тензоры PyTorch\n",
    "X = torch.Tensor(df['text_vector'].tolist())\n",
    "y = torch.LongTensor(df['label'].values)\n",
    "\n",
    "# Разделение данных на обучающий и тестовый наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Создание даталоадеров\n",
    "train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8e8efdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7047659248975529\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.75     24504\n",
      "           1       0.72      0.82      0.77     28247\n",
      "           2       0.64      0.49      0.55      6853\n",
      "           3       0.73      0.62      0.67     11339\n",
      "           4       0.65      0.58      0.61      9376\n",
      "           5       0.59      0.42      0.49      3043\n",
      "\n",
      "    accuracy                           0.71     83362\n",
      "   macro avg       0.68      0.62      0.64     83362\n",
      "weighted avg       0.71      0.71      0.70     83362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Создание экземпляра нейронной сети\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "output_size = len(df['label'].unique())\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Обучение модели\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Оценка модели на тестовых данных\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    \n",
    "# Преобразование тензоров PyTorch обратно в массивы NumPy\n",
    "y_test_numpy = y_test.numpy()\n",
    "predicted_numpy = predicted.numpy()\n",
    "\n",
    "# Вычисление метрик и создание отчета о классификации\n",
    "f1 = f1_score(y_test_numpy, predicted_numpy, average='weighted')\n",
    "report = classification_report(y_test_numpy, predicted_numpy)\n",
    "\n",
    "print(f'F1 Score: {f1}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c6199e",
   "metadata": {},
   "source": [
    "## Вывод\n",
    "\n",
    "### Первая модель:\n",
    "* F1: 0.8988\n",
    "* Точность и полнота высоки для всех классов (в пределах 0.80-0.94).\n",
    "* Макро-средний F1: 0.86\n",
    "* Взвешенный средний F1: 0.90\n",
    "В целом, модель показывает отличные результаты с высокой точностью и полнотой для всех классов. Взвешенный средний F1близок к 1, что указывает на хорошую способность модели обобщать и классифицировать данные.\n",
    "\n",
    "### Вторая модель:\n",
    "* F1: 0.7048\n",
    "* Точность и полнота ниже для всех классов (в пределах 0.59-0.73).\n",
    "* Макро-средний F1: 0.64\n",
    "* Взвешенный средний F1: 0.70\n",
    "Модель во втором случае показывает намного более низкие метрики по сравнению с первым случаем. Она имеет более низкую точность и полноту, что может означать, что она делает больше ошибок в классификации данных.\n",
    "\n",
    "### Анализ:\n",
    "Плохие результаты второй модели можно объяснить разными архитектурами моделей, а так же отсутсвие предобработки данных на второй модели. Повысить результаты второй модели можно, включив предобработку данных, а также изменяя гиперпараметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73acd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
